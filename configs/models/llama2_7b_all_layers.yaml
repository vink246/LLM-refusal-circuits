# Model configuration for Llama-2-7b-chat-hf with ALL decoder layers
# This configuration collects activations from every decoder block for comprehensive analysis

model:
    name: "meta-llama/Llama-2-7b-chat-hf"
    device: "cuda" # or "cpu"
    torch_dtype: "float16" # "float16" or "float32"
    trust_remote_code: false
    cache_dir: null # Optional: HuggingFace cache directory (e.g., "/path/to/hf_cache" or "~/.cache/huggingface")

    # Collect activations from ALL decoder layers
    # Llama-2-7B has 32 layers (0-31)
    activation_layers:
        - "all" # Shorthand for all residual layers (residuals_0 through residuals_31)

    # Alternative options (commented out):
    # - ["residuals_all"]  # Explicit: all residual layers
    # - ["mlp_all"]  # All MLP layers
    # - ["attention_all"]  # All attention layers
    # - ["residuals_0-31"]  # Range specification
    # - ["residuals_0-15", "residuals_16-31"]  # Split into two ranges
    # - ["residuals_all", "mlp_all"]  # Both residuals and MLPs (64 total layers!)

inference:
    batch_size: 2 # REDUCED for memory (32 layers × batch_size × hidden_dim is large!)
    max_new_tokens: 20 # Number of tokens to generate
    save_activations: true

data:
    # Data configuration (can be overridden by --data-config)
    path: "data/raw/or-bench"
    categories: [] # Empty = all available categories
    category_balance_strategy: "use_all" # "use_all" or "equalize"

output:
    result_dir: "results"
# Memory usage estimate for Llama-2-7B (4096 hidden dim):
# - 32 layers × batch_size=2 × 4096 × 4 bytes (float32) ≈ 1 MB per sample
# - For 1000 samples: ~1 GB of activation storage
# - GPU memory during inference: ~8-12 GB (model weights + activations + KV cache)

